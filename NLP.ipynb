{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ce52cf",
   "metadata": {},
   "source": [
    "# 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c475a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'amazing', '!', 'Let', \"'s\", 'explore', 'it', '.']\n",
      "Sentence Tokens: ['Natural Language Processing (NLP) is amazing!', \"Let's explore it.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KIET\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Natural Language Processing (NLP) is amazing! Let's explore it.\"\n",
    "\n",
    "# Word Tokenization\n",
    "word_tokens = word_tokenize(text)\n",
    "print(\"Word Tokens:\", word_tokens)\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "print(\"Sentence Tokens:\", sentence_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fec15e",
   "metadata": {},
   "source": [
    "# 2. Lowercasing in NLP (Text Preprocessing Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f64f06bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', '!', 'nlp', 'is', 'fun', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KIET\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Hello World! NLP is Fun.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Lowercasing each token\n",
    "lower_tokens = [word.lower() for word in tokens]\n",
    "print(lower_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff5c1a9",
   "metadata": {},
   "source": [
    "# 3. Lowercasing Using pandas (Useful for Large Text Datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10305f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Text    Lowercase_Text\n",
      "0      HELLO World!      hello world!\n",
      "1      THIS is NLP.      this is nlp.\n",
      "2  Machine Learning  machine learning\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({'Text': [\"HELLO World!\", \"THIS is NLP.\", \"Machine Learning\"]})\n",
    "data['Lowercase_Text'] = data['Text'].str.lower()\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c475ac46",
   "metadata": {},
   "source": [
    "# 4. STOP WORD REMOVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0383dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words: ['simple', 'example', 'demonstrate', 'stopword', 'removal.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KIET\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "text = \"This is a simple example to demonstrate stopword removal.\"\n",
    "word = text.split()\n",
    "filtered_words = [word for word in word if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Filtered Words:\", filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb4b9f0",
   "metadata": {},
   "source": [
    "# 5. Stemming (Reducing Words to Their Root Form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e3cb3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['natur', 'languag', 'process', '(', 'nlp', ')', 'is', 'amaz', '!', 'let', \"'s\", 'explor', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stemmed_words = [ps.stem(word) for word in word_tokens]\n",
    "\n",
    "print(\"Stemmed Words:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf976b92",
   "metadata": {},
   "source": [
    "# 6. Lemmatization (More Advanced Root Word Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d20ad595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\KIET\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'amazing', '!', 'Let', \"'s\", 'explore', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "\n",
    "print(\"Lemmatized Words:\", lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35fe5b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\KIET\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deab7a3",
   "metadata": {},
   "source": [
    "# 7. Frequency Distribution of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71f91e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Words: [('Natural', 1), ('Language', 1), ('Processing', 1), ('(', 1), ('NLP', 1)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist = FreqDist(word_tokens)\n",
    "print(\"Most Common Words:\", fdist.most_common(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a0ce3b",
   "metadata": {},
   "source": [
    "# 8. Punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00c4e874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Hello, World! NLP is exciting; isn't it?\n",
      "Text Without Punctuation: Hello World NLP is exciting isnt it\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Raw Text (कच्चा पाठ)\n",
    "text = \"Hello, World! NLP is exciting; isn't it?\"\n",
    "\n",
    "# Punctuation Removal\n",
    "text_without_punctuation = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Text Without Punctuation:\", text_without_punctuation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f39c8f2",
   "metadata": {},
   "source": [
    "# 9. Special Character removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "584d552c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Hello, World! This text contains special characters like @, #, $, %, and &.\n",
      "Text Without Special Characters: Hello, World! This text contains special characters like , , , , and .\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Raw Text \n",
    "text = \"Hello, World! This text contains special characters like @, #, $, %, and &.\"\n",
    "\n",
    "# Special Characters Removal\n",
    "text_without_special_chars = re.sub(r'[@#$%&]', '', text)\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Text Without Special Characters:\", text_without_special_chars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a3edc1",
   "metadata": {},
   "source": [
    "# 10. Whitespace Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7b6722f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Hello,     World!   This is   a sample   text.  \n",
      "Text Without Extra Whitespaces: Hello, World! This is a sample text.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Raw Text \n",
    "text = \"Hello,     World!   This is   a sample   text.  \"\n",
    "\n",
    "# Whitespace Removal\n",
    "normalized_text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Text Without Extra Whitespaces:\", normalized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091ed0bf",
   "metadata": {},
   "source": [
    "# 11. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b73add17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\KIET\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\KIET\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities: (S\n",
      "  Natural/JJ\n",
      "  Language/NNP\n",
      "  Processing/NNP\n",
      "  (/(\n",
      "  (ORGANIZATION NLP/NNP)\n",
      "  )/)\n",
      "  is/VBZ\n",
      "  amazing/JJ\n",
      "  !/.\n",
      "  Let/NNP\n",
      "  's/POS\n",
      "  explore/VB\n",
      "  it/PRP\n",
      "  ./.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "ner_tree = ne_chunk(pos_tags)\n",
    "print(\"Named Entities:\", ner_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e15c0d5",
   "metadata": {},
   "source": [
    "# 12. Part-of-Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "778cf339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize and tag\n",
    "tokens = word_tokenize(sentence)\n",
    "tags = pos_tag(tokens)\n",
    "\n",
    "# Display POS tags\n",
    "print(tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37465c83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
